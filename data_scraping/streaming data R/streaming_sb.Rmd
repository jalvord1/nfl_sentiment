---
title: "super bowl 19 stream"
author: "Julianna Alvord"
date: "2/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(httr)
library(twitteR)
library(rtweet)
library(dplyr)
library(readr)
```

#loading in data
```{r}
starters <- read.csv("/Users/juliannaalvord/Documents/nfl sentiment/sb analysis/sb_starters.csv", stringsAsFactors = FALSE)

#pulling out player names
name <- starters$players

#cleaning twitter column, selecting that column, then filtering out those without twitter handle
twitter_clean <- starters %>%
  mutate(twitter_clean = sub("'", "", twitter)) %>%
  select(twitter_clean) %>%
  filter(!twitter_clean == "")

#list of twitter handles
twitter <- twitter_clean$twitter_clean

#full name and twitter handle for streaming
full <- c(name, twitter)
```

#stream key words
```{r}
## Stream keywords used to filter tweets
q <- paste(full, collapse=',' )

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 7 hours
streamtime <- 7 * 60 * 60

## Filename to save json data (backup)
filename <- "/Users/juliannaalvord/Documents/nfl sentiment/sb_tweets.json"
```

#streaming
```{r}
#save as json for later parsing
stream_tweets(q = q, timeout = streamtime, file_name = filename, parse = FALSE, language = "en")
```

#parsing json file
```{r}
#parsing entire file
rt <- parse_stream(filename)

#tweets seem to be missing



##############################



#parsing the json file by halves

#first half
filename2 <- "/Users/juliannaalvord/Documents/nfl sentiment/sb_tweets_half1.json"

#parse those tweets from above
rt <- parse_stream(filename2)

#second half
filename3 <- "/Users/juliannaalvord/Documents/nfl sentiment/sb_tweets_half2.json"

#parse tweets from above
rt2 <- parse_stream(filename3)
```

#writing out/saving files
```{r}
#write_csv(rt, "/Volumes/easystore/Thesis/superbowl_tweets.csv")
#wont work because column lists
#if csv is wanted, no list columns (must unnest)

#saving as rda file instead
save(rt, file = "/Volumes/easystore/Thesis/superbowl_tweets.rda")

#compressing
save(rt, file = "/Volumes/easystore/Thesis/superbowl_tweets_comp.rda", compress = "xz")


###########################


#adding more saves for new loaded files (halves)
save(rt2, file = "/Users/juliannaalvord/Documents/nfl sentiment/sb_tweets_half1.rda")

save(rt3, file = "/Users/juliannaalvord/Documents/nfl sentiment/sb_tweets_half2.rda")
```

#testing a load of rda file
```{r}
load("/Volumes/easystore/Thesis/superbowl_tweets.rda")
```

